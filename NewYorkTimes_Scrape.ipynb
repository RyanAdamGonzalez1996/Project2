{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scrape data from: https://en.m.wikipedia.org/wiki/Lists_of_The_New_York_Times_Fiction_Best_Sellers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Libraries\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import requests\n",
    "import os\n",
    "from splinter import Browser\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get the years best seller table into a list\n",
    "def yearTable_toList():\n",
    "    \n",
    "    # Initialize the Beautiful Soup instance and read the html with pandas\n",
    "    html = browser.html\n",
    "    soup = bs(html, \"html.parser\")\n",
    "    table = pd.read_html(html)\n",
    "    \n",
    "    # Get the First index, which is table on the year page.\n",
    "    df = table[0]\n",
    "\n",
    "    # Format the dataframe to only have unique book titles\n",
    "    df = df.drop(columns = [\"Date\", \"Author\"])\n",
    "    df = df.drop_duplicates()\n",
    "\n",
    "    # Convert the dataframe to a list\n",
    "    bookList = df[\"Book\"].tolist()\n",
    "    \n",
    "    # Return the bookList\n",
    "    return(bookList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the URL\n",
    "nytimes_url = \"https://en.m.wikipedia.org/wiki/Lists_of_The_New_York_Times_Fiction_Best_Sellers\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WDM] - Current google-chrome version is 87.0.4280\n",
      "[WDM] - Get LATEST driver version for 87.0.4280\n",
      "[WDM] - Get LATEST driver version for 87.0.4280\n",
      "[WDM] - Trying to download new driver from http://chromedriver.storage.googleapis.com/87.0.4280.88/chromedriver_win32.zip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WDM] - Driver has been saved in cache [C:\\Users\\ryana\\.wdm\\drivers\\chromedriver\\win32\\87.0.4280.88]\n"
     ]
    }
   ],
   "source": [
    "# Setup the Splinter Instance\n",
    "executable_path = {'executable_path': ChromeDriverManager().install()}\n",
    "browser = Browser('chrome', **executable_path, headless=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ryana\\Anaconda3\\lib\\site-packages\\splinter\\driver\\webdriver\\__init__.py:482: FutureWarning: browser.find_link_by_partial_href is deprecated. Use browser.links.find_by_partial_href instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1931\n",
      "1932\n",
      "1933\n",
      "1934\n",
      "1935\n",
      "1936\n",
      "1937\n",
      "1938\n",
      "1939\n",
      "1940\n",
      "1941\n",
      "1942\n",
      "1943\n",
      "1944\n",
      "1945\n",
      "Error: Couldn't Scrape Data\n",
      "1947\n",
      "1948\n",
      "1949\n",
      "1950\n",
      "1951\n",
      "1952\n",
      "1953\n",
      "1954\n",
      "1955\n",
      "1956\n",
      "1957\n",
      "1958\n",
      "1959\n",
      "1960\n",
      "1961\n",
      "1962\n",
      "1963\n",
      "1964\n",
      "1965\n",
      "1966\n",
      "1967\n",
      "1968\n",
      "1969\n",
      "1970\n",
      "1971\n",
      "1972\n",
      "1973\n",
      "1974\n",
      "1975\n",
      "1976\n",
      "1977\n",
      "1978\n",
      "1979\n",
      "1980\n",
      "1981\n",
      "1982\n",
      "1983\n",
      "1984\n"
     ]
    }
   ],
   "source": [
    "# Declare the dictionary variable\n",
    "bookDict = {}\n",
    "\n",
    "# Loop through the years in the best sellers list\n",
    "for year in range(1931,2021):\n",
    "    try:\n",
    "        # Go back to the main list to continue navigation\n",
    "        browser.visit(nytimes_url)\n",
    "        \n",
    "        # Navigate to each year's page\n",
    "        browser.click_link_by_partial_href(year) \n",
    "        \n",
    "        # Call the yearTable_toList and assign it to the bookList variable\n",
    "        bookList = yearTable_toList()\n",
    "        \n",
    "        # Iterate through the bookList and assign each book with it's best seller year\n",
    "        for book in bookList:\n",
    "            # Append the list to a dictionary with the year as the reference\n",
    "            bookDict[book] = year\n",
    "        \n",
    "        # Output the current year, to keep track of where data has been scraped\n",
    "        print(year)\n",
    "        \n",
    "        # Reset the bookList variable for next sequence\n",
    "        bookList = []\n",
    "\n",
    "    except:\n",
    "        print(\"Error: Couldn't Scrape Data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Close the browser\n",
    "browser.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(bookDict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and format DataFrame of dictionary for better visibility\n",
    "finalBook_df = pd.DataFrame.from_dict(bookDict, orient = \"index\")\n",
    "finalBook_df = finalBook_df.reset_index()\n",
    "finalBook_df = finalBook_df.rename(columns = {\"index\" : \"Book Title\", 0 : \"Best Seller Year\"})\n",
    "finalBook_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the csv file so that we can update with the data scraped for NewYorkTimes bestseller\n",
    "input_file = \"Resources/good_books.csv\"\n",
    "books_df = pd.read_csv(input_file)\n",
    "books_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the bestseller_year and if it matches the title in the dictionar, \n",
    "# update the row value with the year, if not, put N/A\n",
    "books_df[\"bestseller_year\"] = books_df[\"title\"].map(bookDict).fillna(\"N/A\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uses this dataframe to create the \"new_york_times\" table with the columns \"isbn\" and \"bestseller_year\"\n",
    "new_york_times = books_df[[\"isbn\",\"bestseller_year\"]]\n",
    "new_york_times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "engine = create_engine(f'postgresql://{username}:{password}@localhost:5432/good_books_db')\n",
    "connection = engine.connect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the dataframe to the Output folder.\n",
    "books_df.to_sql(\"new_york_times\", con = connection, if_exists = \"append\", index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
